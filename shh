import pandas as pd
import numpy as np
import pickle
import joblib
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from skmultilearn.problem_transform import BinaryRelevance
import re
import neattext.functions as nfx

class MultilabelClassifier:
    def __init__(self):
        self.model = None
        self.tfidf_vectorizer = None
        self.target_cols = None
        self.numeric_feature_names = None
        
    def preprocess_text(self, text):
        """Та же предобработка, что использовалась при обучении"""
        if pd.isna(text) or text == '':
            return ''
        
        text = str(text).lower()
        text = re.sub(r'http\S+|www\S+|https\S+', '', text)
        text = re.sub(r'\S+@\S+', '', text)
        text = re.sub(r'\+?\d[\d\s\-\(\)]{10,}', '', text)
        text = re.sub(r'[^\w\s]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        text = nfx.remove_stopwords(text, lang='ru')
        
        return text.strip()
    
    def create_marker_words_features(self, df):
        """Создание признаков на основе слов-маркеров"""
        marker_words = {
            'ЗП': ['зарплат', 'оклад', 'деньги', 'выплат', 'доход', 'мало', 'недостаточно', 'финанс', 'бюджет'],
            'График': ['график', 'время', 'смен', 'переработк', 'выходн', 'час', 'режим', 'ночн', 'сверхурочн'],
            'Взаимоотношения с руководителем / коллегами': ['начальник', 'руководител', 'коллег', 'отношен', 'конфликт', 'команд', 'общен'],
            'Условия труда': ['условия', 'рабочее место', 'оборудован', 'комфорт', 'температур', 'освещен', 'безопасност'],
            'Отсутствие карьерного роста': ['карьер', 'рост', 'развит', 'повышен', 'перспектив', 'будущ', 'продвижен'],
            'Стресс': ['стресс', 'нервн', 'напряжен', 'устал', 'выгоран', 'психолог', 'давлен', 'беспокойств'],
            'Переезд': ['переезд', 'перееха', 'смен', 'местожительств', 'другой город', 'семь', 'жилье'],
            'Транспортная доступность': ['дорог', 'транспорт', 'добират', 'далеко', 'метро', 'автобус', 'пробк'],
            'Функционал': ['обязанност', 'задач', 'функци', 'работ', 'делать', 'должност', 'функционал'],
            'Социальный пакет': ['соц', 'льгот', 'страховк', 'отпуск', 'больничн', 'компенсац', 'бонус'],
            'Проблемы с адаптацией': ['адаптац', 'привык', 'новичок', 'ориентац', 'влива', 'знаком'],
            'Состояние здоровья сотрудника / родственника': ['здоров', 'болезн', 'лечен', 'врач', 'больниц', 'родственник'],
            'Ушел на другую работу': ['другая работ', 'новое место', 'предложен', 'лучш', 'интересн'],
            'Нет возможности совмещать с учебой': ['учеб', 'институт', 'университет', 'студент', 'экзамен', 'сессия'],
            'Призыв на воинскую службу / уход на СВО': ['армия', 'военн', 'призыв', 'служб', 'сво', 'мобилизац'],
            'Заканчиваются разрешительные документы': ['документ', 'виза', 'разрешен', 'патент', 'регистрац', 'закончился']
        }
        
        df = df.copy()
        
        for category, words in marker_words.items():
            if category in self.target_cols:
                df[f'{category}_marker_count'] = df['Комментарий_processed'].apply(
                    lambda x: sum(1 for word in words if word in x.lower())
                )
                df[f'{category}_has_markers'] = (df[f'{category}_marker_count'] > 0).astype(int)
        
        return df
    
    def create_syntactic_features(self, df):
        """Создание синтаксических признаков"""
        df = df.copy()
        
        df['sentence_count'] = df['Комментарий'].str.count('[.!?]+')
        df['avg_sentence_length'] = df['text_length'] / (df['sentence_count'] + 1)
        df['comma_count'] = df['Комментарий'].str.count(',')
        df['comma_density'] = df['comma_count'] / (df['word_count'] + 1)
        
        conjunctions = ['и', 'а', 'но', 'или', 'либо', 'также', 'тоже', 'либо', 'ни', 'да']
        df['conjunction_count'] = df['Комментарий_processed'].apply(
            lambda x: sum(1 for conj in conjunctions if f' {conj} ' in f' {x} ')
        )
        
        prepositions = ['в', 'на', 'с', 'по', 'за', 'к', 'от', 'для', 'до', 'при', 'под', 'над', 'через']
        df['preposition_count'] = df['Комментарий_processed'].apply(
            lambda x: sum(1 for prep in prepositions if f' {prep} ' in f' {x} ')
        )
        df['preposition_density'] = df['preposition_count'] / (df['word_count'] + 1)
        
        negations = ['не', 'нет', 'ни', 'без', 'отсутств']
        df['negation_count'] = df['Комментарий_processed'].apply(
            lambda x: sum(1 for neg in negations if neg in x.lower())
        )
        
        positive_words = ['хорош', 'отличн', 'замечательн', 'прекрасн', 'нравит', 'понравил', 'доволен']
        df['positive_words_count'] = df['Комментарий_processed'].apply(
            lambda x: sum(1 for word in positive_words if word in x.lower())
        )
        
        negative_words = ['плох', 'ужасн', 'отвратительн', 'не нравит', 'недоволен', 'проблем', 'трудност']
        df['negative_words_count'] = df['Комментарий_processed'].apply(
            lambda x: sum(1 for word in negative_words if word in x.lower())
        )
        
        intensifiers = ['очень', 'крайне', 'чрезвычайно', 'совершенно', 'абсолютно', 'максимально']
        df['intensifier_count'] = df['Комментарий_processed'].apply(
            lambda x: sum(1 for word in intensifiers if word in x.lower())
        )
        
        return df
    
    def create_all_features(self, df):
        """Создание всех признаков для новых данных"""
        df = df.copy()
        
        # Предобработка текста
        df['Комментарий_processed'] = df['Комментарий'].apply(self.preprocess_text)
        
        # Базовые признаки
        df['text_length'] = df['Комментарий'].str.len()
        df['word_count'] = df['Комментарий'].str.split().str.len()
        df['exclamation_count'] = df['Комментарий'].str.count('!')
        df['question_count'] = df['Комментарий'].str.count('\?')
        df['capital_count'] = df['Комментарий'].str.count('[А-ЯЁ]')
        
        # Признаки слов-маркеров
        df = self.create_marker_words_features(df)
        
        # Синтаксические признаки
        df = self.create_syntactic_features(df)
        
        return df
    
    def fit(self, df):
        """Обучение модели"""
        # Определение целевых переменных
        self.target_cols = ['Взаимоотношения с руководителем / коллегами', 'График', 'ЗП', 
                           'Заканчиваются разрешительные документы', 'Нет возможности совмещать с учебой', 
                           'Отсутствие карьерного роста', 'Переезд', 'Призыв на воинскую службу / уход на СВО', 
                           'Проблемы с адаптацией', 'Состояние здоровья сотрудника / родственника', 
                           'Социальный пакет', 'Стресс', 'Транспортная доступность', 'Условия труда', 
                           'Ушел на другую работу', 'Функционал']
        
        # Создание признаков
        df_processed = self.create_all_features(df)
        
        # Векторизация текста
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=10000,
            ngram_range=(1, 3),
            min_df=2,
            max_df=0.95,
            sublinear_tf=True,
            norm='l2'
        )
        
        X_text = self.tfidf_vectorizer.fit_transform(df_processed['Комментарий_processed']).toarray()
        
        # Числовые признаки
        self.numeric_feature_names = [col for col in df_processed.columns 
                                     if col not in ['Комментарий', 'Комментарий_processed', 'Категория'] + self.target_cols]
        X_numeric = df_processed[self.numeric_feature_names].fillna(0).values
        
        # Объединение признаков
        X = np.hstack([X_text, X_numeric])
        y = df[self.target_cols]
        
        # Обучение модели
        self.model = BinaryRelevance(RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42))
        self.model.fit(X, y)
        
        return self
    
    def predict(self, df):
        """Предсказание для новых данных"""
        if self.model is None:
            raise ValueError("Модель не обучена! Сначала вызовите fit()")
        
        # Создание тех же признаков
        df_processed = self.create_all_features(df)
        
        # Векторизация текста (используем уже обученный векторизатор)
        X_text = self.tfidf_vectorizer.transform(df_processed['Комментарий_processed']).toarray()
        
        # Числовые признаки (важно: использовать те же колонки в том же порядке!)
        X_numeric = df_processed[self.numeric_feature_names].fillna(0).values
        
        # Объединение признаков
        X = np.hstack([X_text, X_numeric])
        
        # Предсказание
        predictions = self.model.predict(X)
        probabilities = self.model.predict_proba(X)
        
        # Возвращаем DataFrame с результатами
        result_df = df.copy()
        
        # Добавляем предсказания (0/1)
        for i, col in enumerate(self.target_cols):
            result_df[f'{col}_predicted'] = predictions[:, i]
            result_df[f'{col}_probability'] = probabilities[:, i]
        
        return result_df
    
    def save_model(self, filepath):
        """Сохранение модели"""
        model_data = {
            'model': self.model,
            'tfidf_vectorizer': self.tfidf_vectorizer,
            'target_cols': self.target_cols,
            'numeric_feature_names': self.numeric_feature_names
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"Модель сохранена в {filepath}")
    
    def load_model(self, filepath):
        """Загрузка модели"""
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        self.model = model_data['model']
        self.tfidf_vectorizer = model_data['tfidf_vectorizer']
        self.target_cols = model_data['target_cols']
        self.numeric_feature_names = model_data['numeric_feature_names']
        
        print(f"Модель загружена из {filepath}")
        return self

# ПРИМЕР ИСПОЛЬЗОВАНИЯ:

# 1. ОБУЧЕНИЕ И СОХРАНЕНИЕ МОДЕЛИ
def train_and_save_model():
    """Обучение модели на тренировочных данных"""
    
    # Загрузка обучающих данных
    df_train = pd.read_excel('training_data.xlsx')
    
    # Обработка категорий (как у вас было)
    all_cat = set()
    for cat in df_train['Категория']:
        catgrs = [c.strip() for c in cat.split(';')]
        all_cat.update(catgrs)
    all_cat = sorted(list(all_cat))
    
    for cat in all_cat:
        df_train[cat] = df_train['Категория'].apply(lambda x: 1.0 if cat in [c.strip() for c in x.split(';')] else 0.0)
    
    df_train['Комментарий'] = df_train['Комментарий'].fillna('').astype(str)
    
    # Создание и обучение модели
    classifier = MultilabelClassifier()
    classifier.fit(df_train)
    
    # Сохранение модели
    classifier.save_model('multilabel_model.pkl')
    
    return classifier

# 2. ИСПОЛЬЗОВАНИЕ МОДЕЛИ ДЛЯ НОВЫХ ДАННЫХ
def classify_new_data(new_data_file):
    """Классификация новых данных"""
    
    # Загрузка модели
    classifier = MultilabelClassifier()
    classifier.load_model('multilabel_model.pkl')
    
    # Загрузка новых данных
    # Новые данные должны содержать колонку 'Комментарий'
    df_new = pd.read_excel(new_data_file)
    df_new['Комментарий'] = df_new['Комментарий'].fillna('').astype(str)
    
    # Предсказание
    results = classifier.predict(df_new)
    
    # Сохранение результатов
    results.to_excel('classified_results.xlsx', index=False)
    
    print("Результаты классификации сохранены в 'classified_results.xlsx'")
    
    return results

# 3. ПРИМЕР ИСПОЛЬЗОВАНИЯ ДЛЯ ОДНОГО КОММЕНТАРИЯ
def classify_single_comment(comment_text):
    """Классификация одного комментария"""
    
    # Загрузка модели
    classifier = MultilabelClassifier()
    classifier.load_model('multilabel_model.pkl')
    
    # Создание DataFrame с одним комментарием
    df_single = pd.DataFrame({'Комментарий': [comment_text]})
    
    # Предсказание
    result = classifier.predict(df_single)
    
    # Вывод результатов
    print(f"Комментарий: {comment_text}")
    print("\nПредсказанные категории:")
    
    for col in classifier.target_cols:
        if result[f'{col}_predicted'].iloc[0] == 1:
            prob = result[f'{col}_probability'].iloc[0]
            print(f"  {col}: {prob:.3f}")
    
    return result

# ЗАПУСК:
if __name__ == "__main__":
    # Обучение и сохранение модели (делается один раз)
    print("Обучение модели...")
    train_and_save_model()
    
    # Классификация новых данных (можно запускать много раз)
    print("\nКлассификация новых данных...")
    # classify_new_data('new_comments.xlsx')
    
    # Пример классификации одного комментария
    print("\nПример классификации:")
    classify_single_comment("Увольняюсь из-за маленькой зарплаты и плохих отношений с начальником")


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import chi2_contingency

# Загрузка данных (пример)
# df = pd.read_csv('увольнения.csv')
# Для примера используем те же синтетические данные, что и ранее
# ... (код создания df остается как в предыдущем примере)

# 1. Ключевой этап: классификация увольнений по типу упоминания ЗП
def classify_reasons(row):
    """Классифицирует тип упоминания зарплаты в причинах увольнения"""
    reasons = [row['Причина1'], row['Причина2'], row['Причина3']]
    reasons = [r for r in reasons if pd.notna(r)]  # Убираем NaN
    
    has_salary = 'Низкая ЗП' in reasons
    has_schedule = 'Сокращение смен' in reasons
    
    if has_salary and not has_schedule:
        # Проверяем, что ЗП была единственной причиной
        if len(reasons) == 1:
            return 'Только ЗП'
        else:
            return 'ЗП + другие причины (без графика)'
    
    elif has_salary and has_schedule:
        return 'ЗП + Сокращение смен'
    
    elif not has_salary:
        return 'Без упоминания ЗП'
    
    return 'Неопределено'

# Применяем классификацию
df['Тип_упоминания_ЗП'] = df.apply(classify_reasons, axis=1)

# 2. Глубокий анализ по складам
warehouse_analysis = pd.crosstab(
    df['Склад'], 
    df['Тип_упоминания_ЗП'],
    normalize='index'  # Нормализуем по строкам (проценты внутри склада)
).round(4) * 100

print("\nКЛЮЧЕВОЙ АНАЛИЗ: Как упоминается зарплата по складам (в %)")
print("=========================================================")
print(warehouse_analysis)

# 3. Визуализация - именно то, что вам нужно!
plt.figure(figsize=(14, 8))

# Создаем стекированный барплот
stacked_data = warehouse_analysis[['Только ЗП', 'ЗП + Сокращение смен', 'ЗП + другие причины (без графика)']].copy()

# Сортируем склады по доле проблемного случая (ЗП + Сокращение смен)
stacked_data = stacked_data.sort_values('ЗП + Сокращение смен', ascending=False)

ax = stacked_data.plot(kind='bar', stacked=True, colormap='viridis', edgecolor='black')
plt.title('Структура упоминаний ЗП по складам', fontsize=16)
plt.ylabel('Доля случаев (%)', fontsize=12)
plt.xlabel('Склад', fontsize=12)
plt.xticks(rotation=0)
plt.legend(title='Тип упоминания', bbox_to_anchor=(1.05, 1), loc='upper left')

# Добавляем аннотации с процентами
for p in ax.patches:
    width = p.get_width()
    height = p.get_height()
    x, y = p.get_xy()
    
    # Показываем только значимые значения (от 5%)
    if height > 5:
        ax.annotate(f'{height:.1f}%', (x + width/2, y + height/2), 
                    ha='center', va='center', color='white', fontweight='bold')

plt.tight_layout()
plt.savefig('анализ_зп_по_складам.png', dpi=300, bbox_inches='tight')
plt.show()

# 4. Проверка статистической значимости различий
print("\nСТАТИСТИЧЕСКАЯ ЗНАЧИМОСТЬ РАЗЛИЧИЙ МЕЖДУ СКЛАДАМИ")
print("=================================================")

# Группируем данные для теста
only_salary = df[df['Тип_упоминания_ЗП'] == 'Только ЗП'].groupby('Склад').size()
schedule_issue = df[df['Тип_упоминания_ЗП'] == 'ЗП + Сокращение смен'].groupby('Склад').size()

# Создаем таблицу сопряженности для каждого склада
for warehouse in df['Склад'].unique():
    obs = np.array([
        [only_salary.get(warehouse, 0), schedule_issue.get(warehouse, 0)],
        [len(df[(df['Склад'] == warehouse) & (df['Тип_упоминания_ЗП'] == 'Только ЗП')]), 
         len(df[(df['Склад'] == warehouse) & (df['Тип_упоминания_ЗП'] == 'ЗП + Сокращение смен')])]
    ])
    
    # Проверяем, достаточно ли данных для теста
    if obs.min() >= 5:
        chi2, p, _, _ = chi2_contingency(obs)
        print(f"Склад {warehouse}: p-value = {p:.4f} (Только ЗП vs ЗП+Сокращение)")
        if p < 0.05:
            print(f"  ! Статистически значимые различия (p < 0.05)")

# 5. Глубокий дрил-даун по проблемным складам
print("\nДЕТАЛЬНЫЙ АНАЛИЗ ПРОБЛЕМНЫХ СКЛАДОВ")
print("=====================================")

# Находим склад с максимальной долей "ЗП + Сокращение смен"
problem_warehouse = warehouse_analysis['ЗП + Сокращение смен'].idxmax()
problem_rate = warehouse_analysis.loc[problem_warehouse, 'ЗП + Сокращение смен']

print(f"⚠️ Самый проблемный склад: {problem_warehouse} "
      f"(в {problem_rate:.1f}% случаев увольнения по ЗП связаны с графиком)")

# Анализируем конкретные случаи
problem_cases = df[(df['Склад'] == problem_warehouse) & 
                  (df['Тип_упоминания_ЗП'] == 'ЗП + Сокращение смен')]

print(f"\nПримеры реальных случаев (первые 3 записи):")
print(problem_cases[['ФИО', 'Причина1', 'Причина2', 'Причина3']].head(3))

# 6. Рекомендации по исправлению ситуации
print("\nРЕКОМЕНДАЦИИ ДЛЯ HR-ПОЛИТИКИ")
print("==============================")

if problem_rate > 30:
    print(f"1. На складе {problem_warehouse} критическая ситуация:")
    print(f"   - В {problem_rate:.1f}% случаев увольнения по 'низкой ЗП' на самом деле проблема в графике")
    print("   - Рекомендуется срочно провести анализ политики расписания смен")
    print("   - Внедрить прозрачную систему расчета ЗП с учетом отработанных часов")
else:
    print(f"1. На складе {problem_warehouse} умеренная проблема:")
    print(f"   - В {problem_rate:.1f}% случаев увольнения по 'низкой ЗП' связана с графиком")
    print("   - Рекомендуется улучшить коммуникацию политики расписания с сотрудниками")

print("\n2. Для всех складов:")
print("   - При увольнении фиксировать НЕ ТОЛЬКО основную причину, но и контекст")
print("   - Внедрить диагностический вопрос: 'Как график работы повлиял на ваше решение?'")
print("   - Провести обучение руководителей: как выявлять скрытые причины увольнения")
